_wandb:
    value:
        cli_version: 0.21.3
        e:
            bbmll9yv0dm3jfbk4uvtkoxya1pp6dz9:
                args:
                    - --depth=34
                    - --run=
                    - --bidirectional=True
                    - --use_lora=True
                    - --mask_ratio=0.15
                cpu_count: 30
                cpu_count_logical: 30
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "1456701079552"
                        used: "45165158400"
                email: alexander.ulanch@colorado.edu
                executable: /home/ubuntu/nano-embed/.venv/bin/python3
                git:
                    commit: d655638c980d7b071d76dc47859f008ebec10a2d
                    remote: https://github.com/ulanch/nano-embed.git
                gpu: NVIDIA A10
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 9216
                      memoryTotal: "24146608128"
                      name: NVIDIA A10
                      uuid: GPU-24de0669-284a-5dd3-1396-22bc8c269d9c
                host: 192-9-128-225
                memory:
                    total: "238546460672"
                os: Linux-6.8.0-1040-nvidia-x86_64-with-glibc2.35
                program: -m scripts.base_train_mntp
                python: CPython 3.10.12
                root: /home/ubuntu/nano-embed
                startedAt: "2025-12-11T08:51:55.537611Z"
                writerId: bbmll9yv0dm3jfbk4uvtkoxya1pp6dz9
        m: []
        python_version: 3.10.12
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 16
            "4": 3.10.12
            "5": 0.21.3
            "12": 0.21.3
            "13": linux-x86_64
bidirectional:
    value: true
core_metric_every:
    value: 2000
core_metric_max_per_task:
    value: 500
depth:
    value: 34
device_batch_size:
    value: 32
device_type:
    value: ""
embedding_lr:
    value: 0.2
eval_every:
    value: 250
eval_tokens:
    value: 10485760
final_lr_frac:
    value: 0
grad_clip:
    value: 1
lora_alpha:
    value: 32
lora_dropout:
    value: 0
lora_rank:
    value: 4
mask_ratio:
    value: 0.15
matrix_lr:
    value: 0.02
max_seq_len:
    value: 2048
model_tag:
    value: ""
num_iterations:
    value: -1
resume_from_step:
    value: -1
run:
    value: ""
sample_every:
    value: 2000
save_every:
    value: -1
target_flops:
    value: -1
target_param_data_ratio:
    value: 20
total_batch_size:
    value: 524288
unembedding_lr:
    value: 0.004
use_lora:
    value: true
warmdown_ratio:
    value: 0.2
warmup_ratio:
    value: 0
weight_decay:
    value: 0
