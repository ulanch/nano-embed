Vocab size: 65,536
num_layers: 34
model_dim: 2176
num_heads: 17
num_kv_heads: 17
Tokens / micro-batch / rank: 32 x 2048 = 65,536
Tokens / micro-batch: 65,536
Total batch size 524,288 => gradient accumulation steps: 8
Number of parameters: 2,222,680,576
Estimated FLOPs per token: 1.429868e+10
Calculated number of iterations from target data:param ratio: 84,788
Total number of training tokens: 44,453,330,944
Tokens : Params ratio: 20.00
Total training FLOPs estimate: 6.356238e+20
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/nano-embed/scripts/base_train_mntp.py", line 179, in <module>
    optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)
  File "/home/ubuntu/nano-embed/nano_embed/gpt.py", line 267, in setup_optimizers
    adam_groups.append(dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale))
UnboundLocalError: local variable 'dmodel_lr_scale' referenced before assignment
[rank0]: Traceback (most recent call last):
[rank0]:   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank0]:     return _run_code(code, main_globals, None,
[rank0]:   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[rank0]:     exec(code, run_globals)
[rank0]:   File "/home/ubuntu/nano-embed/scripts/base_train_mntp.py", line 179, in <module>
[rank0]:     optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)
[rank0]:   File "/home/ubuntu/nano-embed/nano_embed/gpt.py", line 267, in setup_optimizers
[rank0]:     adam_groups.append(dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale))
[rank0]: UnboundLocalError: local variable 'dmodel_lr_scale' referenced before assignment
